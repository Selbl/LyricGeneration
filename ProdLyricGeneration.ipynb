{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588d093b-ff12-43ea-a884-ad95b35dc760",
   "metadata": {},
   "source": [
    "# Lyric Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a983ff8-82d2-4fa8-a325-99dd9064ced8",
   "metadata": {},
   "source": [
    "The code in this notebook generates the lyrics using the model from the previous notebooks. The code works by receiving a user input and then generating a continuation of the user's prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d860e9c-7538-4102-b62c-bdae2f4bb6a7",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6c5b2-7c6d-40b6-9f5b-24d38c081ded",
   "metadata": {},
   "source": [
    "Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3efed525-9f02-4ac8-8af9-1c219a287966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selbl/anaconda3/envs/CS129/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2Model, GPT2PreTrainedModel\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_built() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe873f92-0b27-44b7-8236-24ccf5f80fee",
   "metadata": {},
   "source": [
    "Load the model and define the generation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22856170-9f3d-4261-bd86-47c4c63c92f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2_Model(GPT2PreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.transformer = GPT2Model.from_pretrained('gpt2')\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|pad|>')\n",
    "\n",
    "        # this is necessary since we add a new unique token for pad_token\n",
    "        self.transformer.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, len(tokenizer), bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "\n",
    "        x = self.transformer(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n",
    "        x = self.lm_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac08ac6d-b67e-4de9-b374-cc48b0482693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "configuration = GPT2Config()\n",
    "gpt_model = GPT2_Model(configuration).to(device)\n",
    "gpt_model.load_state_dict(torch.load('GPT-Trained-Model'))\n",
    "gpt_model.eval()\n",
    "#Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|pad|>') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d763a468-6c6e-47ec-91be-6337e1cc0ab7",
   "metadata": {},
   "source": [
    "Define the generation function. The model samples probable continuations to the user's input. You can play with the top_k and top_p parameters to change the outputs of the model. Lowering p and increasing k makes it so the model outputs more \"out there\" word combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f6cf30-5184-4070-887c-199d96e10826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dedfine generation function\n",
    "def generate(idx, max_new_tokens, context_size, tokenizer, model, top_k=10, top_p=0.95):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            if idx[:,-1].item() != tokenizer.encode(tokenizer.eos_token)[0]:\n",
    "                # crop idx to the last block_size tokens\n",
    "                idx_cond = idx[:, -context_size:]\n",
    "                # get the predictions\n",
    "                logits = model(idx_cond)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :]\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                # sort probabilities in descending order\n",
    "                sorted_probs, indices = torch.sort(probs, descending=True)\n",
    "                # compute cumsum of probabilities\n",
    "                probs_cumsum = torch.cumsum(sorted_probs, dim=1)\n",
    "                # choose only top_p tokens\n",
    "                sorted_probs, indices = sorted_probs[:, :probs_cumsum[[probs_cumsum < top_p]].size()[0] + 1], indices[:, :probs_cumsum[[probs_cumsum < top_p]].size()[0] +1]\n",
    "                # choose only top_k tokens\n",
    "                sorted_probs, indices = sorted_probs[:,:top_k], indices[:,:top_k]\n",
    "                # sample from the distribution\n",
    "                sorted_probs = F.softmax(sorted_probs, dim=-1)\n",
    "                idx_next = indices[:, torch.multinomial(sorted_probs, num_samples=1)].squeeze(0)\n",
    "                # append new token ids\n",
    "                idx = torch.cat((idx, idx_next), dim=1)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c0019-5f41-401d-8115-0c6646976dda",
   "metadata": {},
   "source": [
    "## Lyric Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0860f69-7317-48c5-af31-c8a1f0f64f42",
   "metadata": {},
   "source": [
    "After all the preliminaries, it is time to generate some lyrics! You can change the prompt variable to a string that the model will try to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b12daf-6af7-459e-bc5b-b01e98bbd74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i stand up. cham spreadrefart.2 hill country woman pind, p sit up we fair there steady singin' but no funky park folks walk keep tonight avenue blues i give myself long've it been blond hair high hair floet john pap ah man i dream good boy i wish i was cold ain't got the heart i dream good boy i wish i was cold i'm old kav're i wish i was cold i'm old kbetter live christ i wish i was cold chalt live christ i wish i was coldy rock i wish i was cool listen me wallc differentÐµ so much  [ blow who said destroy yourself do not move open go open anyone dare fleetwood mac liveget tickets as low as $38[ blow who say goodbye away live frank liv freeze that seems y'all make yourself good boy i wish i was cold i'm old k dollar, makes me world american i wish i was cold i'm old k accept angry choices i wish i was\n"
     ]
    }
   ],
   "source": [
    "gpt_model.eval()\n",
    "\n",
    "prompt = \"i stand up\"\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = generated.to(device)\n",
    "\n",
    "sample_outputs = generate(generated,\n",
    "                         max_new_tokens=200,\n",
    "                         context_size=400,\n",
    "                         tokenizer=tokenizer,\n",
    "                         model=gpt_model,\n",
    "                         top_k=10,\n",
    "                         top_p=0.95)\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(f\"{tokenizer.decode(sample_output, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740168c5-f466-4d34-a3a3-39eb8c19d73f",
   "metadata": {},
   "source": [
    "If you care about song structure, you can start the prompt with _[verse]_ (or _[chorus]_ if you are feeling more adventurous) so that the model has a higher chance of outputting more structured lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6438b14c-a0e8-4ec1-8b55-559e37d50161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[verse] i stand up early 'cause i like living in or around the garden gin for my raincoat no more it's at home in a sunday's air because i ain't no fighter a cousin he came home on a jethead that's fine i hear him smoky 'cause he can't control his music  [verse] he grown up, woke up in the pan though he won't light the horse, he's no good although he's having a lot to confess he's no beast, he's sorry to heal he comes home, feeling like japanese water he's laughing in the ice because he can't control his music  [chorus] he'll say he's cute and he'll show you plenty and he won't cry when you leave his door on a fling i might picking, gonna watch you run  [verse] some folks get up singing, some folks get in south rock there's no sensation till you reach the border until you've much created the fruit lable thistle\n"
     ]
    }
   ],
   "source": [
    "gpt_model.eval()\n",
    "\n",
    "prompt = \"[verse] i stand up\"\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = generated.to(device)\n",
    "\n",
    "sample_outputs = generate(generated,\n",
    "                         max_new_tokens=200,\n",
    "                         context_size=400,\n",
    "                         tokenizer=tokenizer,\n",
    "                         model=gpt_model,\n",
    "                         top_k=10,\n",
    "                         top_p=0.95)\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(f\"{tokenizer.decode(sample_output, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece5452-0f38-4dd9-83de-22a625225844",
   "metadata": {},
   "source": [
    "If you care about formatting, you can output your generated songs using the following function. Note that this only works with songs that have a song structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8055eeac-5bb2-4d4d-b97a-c7c2a3b6e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Define capitalization functions\n",
    "def custom_capitalize(match):\n",
    "    return match.group(1).capitalize()\n",
    "\n",
    "def capitalize_string(input_string):\n",
    "    # Capitalize every first letter after \"\\n\\n \"\n",
    "    result = re.sub(r'\\n\\n\\s*([a-zA-Z])', lambda x: '\\n\\n' + x.group(1).upper(), input_string)\n",
    "    # Capitalize every first letter in every word inside brackets ([ ])\n",
    "    result = re.sub(r'\\[([^\\]]*)\\]', lambda x: '[' + ' '.join(word.capitalize() for word in x.group(1).split()) + ']', result)\n",
    "    # Capitalize every instance of the letter i by itself\n",
    "    result = re.sub(r'\\bi\\b', 'I', result)\n",
    "    return result\n",
    "\n",
    "#Format the string\n",
    "def format_string(input_string):\n",
    "    inside_brackets = False\n",
    "    result = ''.join([char + ('\\n\\n' if char == ']' else '') for char in input_string.replace('\\n', '')])\n",
    "    result = result.replace('[', '\\n\\n[')\n",
    "    #Capitalize for good measure\n",
    "    result = capitalize_string(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a65e62-d23e-4c9d-a959-e8fabe73332f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Verse]\n",
      "\n",
      "I stand up early 'cause I like living in or around the garden gin for my raincoat no more it's at home in a sunday's air because I ain't no fighter a cousin he came home on a jethead that's fine I hear him smoky 'cause he can't control his music  \n",
      "\n",
      "[Verse]\n",
      "\n",
      "He grown up, woke up in the pan though he won't light the horse, he's no good although he's having a lot to confess he's no beast, he's sorry to heal he comes home, feeling like japanese water he's laughing in the ice because he can't control his music  \n",
      "\n",
      "[Chorus]\n",
      "\n",
      "He'll say he's cute and he'll show you plenty and he won't cry when you leave his door on a fling I might picking, gonna watch you run  \n",
      "\n",
      "[Verse]\n",
      "\n",
      "Some folks get up singing, some folks get in south rock there's no sensation till you reach the border until you've much created the fruit lable thistle\n"
     ]
    }
   ],
   "source": [
    "lyric = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "lyric = format_string(lyric)\n",
    "print(lyric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51349d9d-954f-44cb-990a-2ac7cde16477",
   "metadata": {},
   "source": [
    "It looks more and more like actual lyrics! I do not include line breaks for each line in each song section because that all depends on the song structure, and so it gives the user more freedom to decide how to structure their song."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0f4d4-ddb9-4c6a-bb16-0c91ac1bdec7",
   "metadata": {},
   "source": [
    "I hope this model works for you! There's a lot to explore by generating lyrics with this method. While this generator is far from perfect, it can give you a great starting point for your next composition. If you come up with anything, please send it my way to give it a spin!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
