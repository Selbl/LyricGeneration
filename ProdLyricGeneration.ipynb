{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588d093b-ff12-43ea-a884-ad95b35dc760",
   "metadata": {},
   "source": [
    "# Lyric Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a983ff8-82d2-4fa8-a325-99dd9064ced8",
   "metadata": {},
   "source": [
    "The code in this notebook generates the lyrics using the model from the previous notebooks. The code works by receiving a user input and then generating a continuation of the user's prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d860e9c-7538-4102-b62c-bdae2f4bb6a7",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6c5b2-7c6d-40b6-9f5b-24d38c081ded",
   "metadata": {},
   "source": [
    "Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3efed525-9f02-4ac8-8af9-1c219a287966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selbl/anaconda3/envs/CS129/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2Model, GPT2PreTrainedModel\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_built() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe873f92-0b27-44b7-8236-24ccf5f80fee",
   "metadata": {},
   "source": [
    "Load the model and define the generation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22856170-9f3d-4261-bd86-47c4c63c92f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2_Model(GPT2PreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.transformer = GPT2Model.from_pretrained('gpt2')\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|pad|>')\n",
    "\n",
    "        # this is necessary since we add a new unique token for pad_token\n",
    "        self.transformer.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, len(tokenizer), bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "\n",
    "        x = self.transformer(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n",
    "        x = self.lm_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac08ac6d-b67e-4de9-b374-cc48b0482693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "configuration = GPT2Config()\n",
    "gpt_model = GPT2_Model(configuration).to(device)\n",
    "gpt_model.load_state_dict(torch.load('GPT-Trained-Model'))\n",
    "gpt_model.eval()\n",
    "#Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|pad|>') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d763a468-6c6e-47ec-91be-6337e1cc0ab7",
   "metadata": {},
   "source": [
    "Define the generation function. The model samples probable continuations to the user's input. You can play with the top_k and top_p parameters to change the outputs of the model. Lowering p and increasing k makes it so the model outputs more \"out there\" word combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f6cf30-5184-4070-887c-199d96e10826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dedfine generation function\n",
    "def generate(idx, max_new_tokens, context_size, tokenizer, model, top_k=10, top_p=0.95):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            if idx[:,-1].item() != tokenizer.encode(tokenizer.eos_token)[0]:\n",
    "                # crop idx to the last block_size tokens\n",
    "                idx_cond = idx[:, -context_size:]\n",
    "                # get the predictions\n",
    "                logits = model(idx_cond)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :]\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                # sort probabilities in descending order\n",
    "                sorted_probs, indices = torch.sort(probs, descending=True)\n",
    "                # compute cumsum of probabilities\n",
    "                probs_cumsum = torch.cumsum(sorted_probs, dim=1)\n",
    "                # choose only top_p tokens\n",
    "                sorted_probs, indices = sorted_probs[:, :probs_cumsum[[probs_cumsum < top_p]].size()[0] + 1], indices[:, :probs_cumsum[[probs_cumsum < top_p]].size()[0] +1]\n",
    "                # choose only top_k tokens\n",
    "                sorted_probs, indices = sorted_probs[:,:top_k], indices[:,:top_k]\n",
    "                # sample from the distribution\n",
    "                sorted_probs = F.softmax(sorted_probs, dim=-1)\n",
    "                idx_next = indices[:, torch.multinomial(sorted_probs, num_samples=1)].squeeze(0)\n",
    "                # append new token ids\n",
    "                idx = torch.cat((idx, idx_next), dim=1)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c0019-5f41-401d-8115-0c6646976dda",
   "metadata": {},
   "source": [
    "## Lyric Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5eee3-7800-4468-b82a-6010a412ecc9",
   "metadata": {},
   "source": [
    "Having done all of the above, the code can now generate lyrics. Before doing so, and in order to keep things PG, I add a profanity filter (which you can disable at your own risk by changing the profanity boolean variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b48438f-f94b-49d8-935e-e8ecece66cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from better_profanity import profanity\n",
    "profanity.load_censor_words()\n",
    "prof = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0860f69-7317-48c5-af31-c8a1f0f64f42",
   "metadata": {},
   "source": [
    "By default, the code adds a _[verse]_ tag to precede the user's prompt as well as block profanity. You can disable that prompting by changing the parts variable below. Keep in mind that the code formats the lyrics to make them more readable only if they include these parts (see the example below).\n",
    "\n",
    "If you would like, you can also change the song so it does not start with a _[verse]_, but rather with _[chorus]_ or something more creative. You can change the prefix variable below for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40318b7e-064b-45e0-921e-d3cf10f454db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set model parameters\n",
    "parts = True\n",
    "prefix = '[verse]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e1c0a2-8b55-4d9c-a854-3b863434c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input prompt\n",
    "prompt = \"I stare at the sun\"\n",
    "prompt = prompt.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b64ee6-8169-4f4c-a4b7-8bb947a49caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process prompt\n",
    "if parts:\n",
    "    prompt = prefix + ' ' + prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc441d34-bfc6-4ef4-84d6-c559a1470024",
   "metadata": {},
   "source": [
    "If you care about formatting, you can output your generated songs using the following function. Note that this only works with songs that have a song structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90ca649-2c98-407f-a72b-c73fc96e879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Define capitalization functions\n",
    "def custom_capitalize(match):\n",
    "    return match.group(1).capitalize()\n",
    "\n",
    "def capitalize_string(input_string):\n",
    "    # Capitalize every first letter after \"\\n\\n \"\n",
    "    result = re.sub(r'\\n\\n\\s*([a-zA-Z])', lambda x: '\\n\\n' + x.group(1).upper(), input_string)\n",
    "    # Capitalize every first letter in every word inside brackets ([ ])\n",
    "    result = re.sub(r'\\[([^\\]]*)\\]', lambda x: '[' + ' '.join(word.capitalize() for word in x.group(1).split()) + ']', result)\n",
    "    # Capitalize every instance of the letter i by itself\n",
    "    result = re.sub(r'\\bi\\b', 'I', result)\n",
    "    return result\n",
    "\n",
    "#Format the string\n",
    "def format_string(input_string):\n",
    "    inside_brackets = False\n",
    "    result = ''.join([char + ('\\n\\n' if char == ']' else '') for char in input_string.replace('\\n', '')])\n",
    "    result = result.replace('[', '\\n\\n[')\n",
    "    #Capitalize for good measure\n",
    "    result = capitalize_string(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5193e9ef-adc5-4f74-b1b1-2efc58971909",
   "metadata": {},
   "source": [
    "## Lyric Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e1dabe-226e-43ab-ac28-0bca1b824111",
   "metadata": {},
   "source": [
    "After all the preliminaries, it is time to generate some lyrics! The following block generates the lyrics. Play around with different prompts and try tweaking all of the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b12daf-6af7-459e-bc5b-b01e98bbd74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model.eval()\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = generated.to(device)\n",
    "\n",
    "sample_outputs = generate(generated,\n",
    "                         max_new_tokens=200,\n",
    "                         context_size=400,\n",
    "                         tokenizer=tokenizer,\n",
    "                         model=gpt_model,\n",
    "                         top_k=10,\n",
    "                         top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "145851c4-923f-41ad-98a3-cadfb44c8e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Verse]\n",
      "\n",
      "I stare at the sun every night up ahead, oh, look at the clouds all these clouds under the hood  \n",
      "\n",
      "[Pre-chorus]\n",
      "\n",
      "They cover my face with bad cheap leather and then shake my hat and wash my arm then shake my hands  \n",
      "\n",
      "[Chorus]\n",
      "\n",
      "Now I find myself in the back of a silver highway and the skies are green and the nights are blue and I'm walking up that highway I body go jumping, he calls, he calls a laughing back  \n",
      "\n",
      "[Verse]\n",
      "\n",
      "Now my shoes are gray and my head goes brown but I'm not joking about the awful talking in the back yard, hey now he did it to you the gocks all my hands are dragging, they whisper, \"we'll save you who you want to be you're being alright with pretending not stealing how you like that  \n",
      "\n",
      "[Pre-chorus]\n",
      "\n",
      "But the crowd can't forget you the big seats are falling down and the kids are putting it down  \n",
      "\n",
      "[Chorus]\n",
      "\n",
      "Now I find myself in the back of\n"
     ]
    }
   ],
   "source": [
    "#Store the lyrics and decode\n",
    "lyric = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "#Format if it has parts\n",
    "if parts:\n",
    "    lyric = format_string(lyric)\n",
    "#Remove profanity\n",
    "if not prof:\n",
    "    lyric = profanity.censor(lyric)\n",
    "print(lyric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0500db-e5f0-46ff-a24c-2366e871cf36",
   "metadata": {},
   "source": [
    "I hope this model works for you! There's a lot to explore by generating lyrics with this method. While this generator is far from perfect, it can give you a great starting point for your next composition. If you come up with anything, please send it my way to give it a spin!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
